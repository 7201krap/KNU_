{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seq2seq.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMLrKx+Aw0Z9I3MxlW5uWI+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xvU_54snuoNx"},"outputs":[],"source":["!python -m spacy download en\n","!python -m spacy download de"]},{"cell_type":"code","source":["import spacy\n","\n","spacy_en = spacy.load('en')\n","spacy_de = spacy.load('de')"],"metadata":{"id":"HMw99NWTHsCT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spacy_en"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0VeYw0XAIYhH","executionInfo":{"status":"ok","timestamp":1642658773368,"user_tz":-540,"elapsed":15,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"b981326e-8ee6-4e0b-99b7-9d3cf9a7afe6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<spacy.lang.en.English at 0x7f4266a8bd90>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# tokenization \n","\n","tokenized = spacy_en.tokenizer(\"I am a graduate student\")\n","\n","for i, token in enumerate(tokenized):\n","    print(f\"index {i}: {token.text}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCwX2NmSIAO1","executionInfo":{"status":"ok","timestamp":1642658773369,"user_tz":-540,"elapsed":13,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"681c9e70-ca85-4f09-fcc3-6a943e801c22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["index 0: I\n","index 1: am\n","index 2: a\n","index 3: graduate\n","index 4: student\n"]}]},{"cell_type":"code","source":["tokenized"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x-5ao0LmJZyP","executionInfo":{"status":"ok","timestamp":1642658773369,"user_tz":-540,"elapsed":10,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"06319e8b-2ed1-4597-e7cc-495f21873600"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["I am a graduate student"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# 우리는 독일어를 영어로 번역하는 task를 하고있다. \n","\n","def tokenize_de(text):\n","    '''\n","    text를 받아서 array형태로 만든다. 그 array는 reversed된 array이다.\n","    '''\n","    return [token.text for token in spacy_de.tokenizer(text)][::-1] # source data를 reverse해준다. source == input\n","\n","def tokenize_en(text):\n","    '''\n","    text를 받아서 array형태로 만든다. 이 array는 reversed되지 않았다. \n","    '''\n","    return [token.text for token in spacy_en.tokenizer(text)]       # target data는 reverse를 해주지 않는다. target == output"],"metadata":{"id":"TscYax5QIU0b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -U torchtext==0.8.0\n","\n","# 그냥 전처리임 \n","# Field 라이브러리를 이용하여 데이터셋에 대한 구체적인 전처리 내용을 명시 \n","from torchtext.data import Field, BucketIterator\n","\n","SRC = Field(tokenize=tokenize_de, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True)   # source = 독일어\n","TRG = Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True)   # target = 영어 "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rHwQe4Z_JgKZ","executionInfo":{"status":"ok","timestamp":1642658788848,"user_tz":-540,"elapsed":3002,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"558efb88-a794-4b86-e3a2-2c04a98fdf9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchtext==0.8.0 in /usr/local/lib/python3.7/dist-packages (0.8.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (4.62.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (2.23.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.10.0+cu111)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.8.0) (3.10.0.2)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"]}]},{"cell_type":"code","source":["from torchtext.datasets import Multi30k\n","\n","train_dataset, valid_dataset, test_dataset = Multi30k.splits(exts=(\".de\", \".en\"), fields=(SRC, TRG))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2ChFkGTJlJj","executionInfo":{"status":"ok","timestamp":1642658798565,"user_tz":-540,"elapsed":9722,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"df8d7831-08b8-4862-8c60-f8c8df23c76c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["downloading training.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 810kB/s] \n"]},{"output_type":"stream","name":"stdout","text":["downloading validation.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 250kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["downloading mmt_task1_test2016.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 238kB/s]\n","/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"]}]},{"cell_type":"code","source":["print(\"len(train_dataset.examples)\", len(train_dataset.examples))\n","print(\"len(validation_dataset.examples)\", len(valid_dataset.examples))\n","print(\"len(test_dataset.examples)\", len(test_dataset.examples))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MAUc1cqhM44b","executionInfo":{"status":"ok","timestamp":1642658798566,"user_tz":-540,"elapsed":8,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"3fde0007-d322-474a-f8ba-45cd6c2b1b3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["len(train_dataset.examples) 29000\n","len(validation_dataset.examples) 1014\n","len(test_dataset.examples) 1000\n"]}]},{"cell_type":"code","source":["# 학습 데이터 중 하나를 선택해 출력해 본다.\n","print(vars(train_dataset.examples[10])['src'])  # source이기 때문에 reversed 된 것을 확인 할 수 있다.\n","print(vars(train_dataset.examples[10])['trg'])  # target이기 때문에 reversed가 되지 않은 것을 확인 할 수 있다. "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8UhdOXrsNznL","executionInfo":{"status":"ok","timestamp":1642658798566,"user_tz":-540,"elapsed":6,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"9f444b99-5114-4fd2-d927-ed67ef552d9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['.', 'springen', 'nacheinander', 'die', ',', 'mädchen', 'fünf', 'mit', 'ballettklasse', 'eine']\n","['a', 'ballet', 'class', 'of', 'five', 'girls', 'jumping', 'in', 'sequence', '.']\n"]}]},{"cell_type":"code","source":["# Field객체의 build_vocab메서드를 활용하여 영어와 독어의 단어 사전을 생성. 최소 2번 이상 등장한 단어만을 사용 \n","\n","SRC.build_vocab(train_dataset, min_freq=2)\n","TRG.build_vocab(train_dataset, min_freq=2)\n","\n","print(\"len(SRC.vocab):\", len(SRC.vocab), \"단어가 2번 나옴\")\n","print(\"len(TRG.vocab):\", len(TRG.vocab), \"단어가 2번 나옴\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"atLoP7hrP3oy","executionInfo":{"status":"ok","timestamp":1642658799050,"user_tz":-540,"elapsed":488,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"9c79facd-63cf-43af-d55e-a68470538c2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["len(SRC.vocab): 7855 단어가 2번 나옴\n","len(TRG.vocab): 5893 단어가 2번 나옴\n"]}]},{"cell_type":"code","source":["# Targets\n","\n","print(TRG.vocab.stoi[\"abcabc\"])         # 없는 단어는 0으로 설정해줌\n","print(TRG.vocab.stoi[TRG.pad_token])    # padding: 1\n","print(TRG.vocab.stoi[\"<sos>\"])          # 2\n","print(TRG.vocab.stoi[\"<eos>\"])          # 3\n","print(TRG.vocab.stoi[\"hello\"])          # \n","print(TRG.vocab.stoi[\"world\"])          # "],"metadata":{"id":"e04FKnwDR9Ub","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642658799051,"user_tz":-540,"elapsed":37,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"888535de-1c2a-4ced-fd1b-485f27a47145"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4112\n","1752\n"]}]},{"cell_type":"code","source":["# Sources\n","\n","print(SRC.vocab.stoi[\"abcabc\"])         # 없는 단어는 0으로 설정해줌\n","print(SRC.vocab.stoi[SRC.pad_token])    # padding: 1\n","print(SRC.vocab.stoi[\"<sos>\"])          # 2\n","print(SRC.vocab.stoi[\"<eos>\"])          # 3\n","print(SRC.vocab.stoi[\"eine\"])           # "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hwYECAq8J3MN","executionInfo":{"status":"ok","timestamp":1642658799051,"user_tz":-540,"elapsed":29,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"7a5dba8a-e188-46cd-a9e8-bcf706337329"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","8\n"]}]},{"cell_type":"code","source":["import torch"],"metadata":{"id":"14qxAejCJ6FE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","BATCH_SIZE = 128 \n","\n","# 한문장에 포함된 단어가 연속적으로 LSTM에 입력 되어야 한다.\n","# 따라서 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하도록 만들면 좋다.\n","# 이를 위해 BucketIterator를 사용한다.\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (\n","        train_dataset,\n","        valid_dataset,\n","        test_dataset,\n","    ),\n","    batch_size = BATCH_SIZE,\n","    device = device\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fF4G5VOXK9Ws","executionInfo":{"status":"ok","timestamp":1642658799052,"user_tz":-540,"elapsed":22,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"03d388b0-275b-4cb7-ecac-6088d414aef0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"]}]},{"cell_type":"code","source":["for i, batch in enumerate(train_iterator):\n","    src = batch.src \n","    trg = batch.trg \n","\n","    print(\"=========0th sentence=========\")\n","    for i in range(src.shape[0]):\n","        print(src[i][0].item())    # 0 번째 문장 \n","    \n","    print(\"=========12th sentence=========\")\n","    for i in range(src.shape[0]):\n","        print(src[i][12].item())   # 12번째 문장\n","\n","    # 첫번째 배치만 확인해 준다.\n","    break"],"metadata":{"id":"LgtCIVQDLWRr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642658809613,"user_tz":-540,"elapsed":10576,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"bb898f9a-7fef-414f-c68d-490626b678a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["=========0th sentence=========\n","2\n","4\n","0\n","15\n","59\n","6996\n","23\n","1263\n","25\n","492\n","3\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","=========12th sentence=========\n","2\n","4\n","245\n","22\n","2180\n","58\n","60\n","16\n","8\n","3\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n","1\n"]}]},{"cell_type":"code","source":["import torch.nn as nn "],"metadata":{"id":"SQggn3hQNJNr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Encoder\n","\n","- 주어진 소스 문장을 context vector로 인코딩 \n","- hidden state와 cell state를 반환 \n"],"metadata":{"id":"8icEu9DzVm03"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n","        super().__init__()    # nn.Module에 있는 모든 '__init__에 정의되어 있는 것들'을 가져와준다. \n","\n","        # embedding layer \n","        # embedding은 one-hot encoding을 특정 차원의 임베딩으로 매핑하는 레이어 \n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","\n","        # LSTM layer \n","        self.hidden_dim = hidden_dim \n","        self.n_layers   = n_layers \n","        self.rnn        = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n","\n","        # dropout\n","        self.dropout = nn.Dropout(dropout_ratio)\n","\n","    def forward(self, src):\n","\n","        # src:[단어 개수, 배치 크기] --> embedded:[단어 개수, 배치 크기, embedded]\n","        embedded = self.dropout(self.embedding(src))\n","\n","        # embedded --> RNN\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","        # outputs:[단어 개수, 배치 크기, 히든 차원]. 현재 단어의 출력 정보 \n","        # hidden:[레이어 개수, 배치 크기, 히든 차원]. 현재까지의 모든 단어의 정보 \n","        # cell:[레이어 개수, 배치 크기, 히든 차원]. 현재까지의 모든 단어의 정보 \n","        \n","        # context vector를 반환 \n","        return hidden, cell "],"metadata":{"id":"0duEpivR4dxw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Decoder\n","\n","- context vector를 타겟 문장으로 디코딩 \n","- hidden state, cell state, prediction을 반환"],"metadata":{"id":"yWITosSHWBHN"}},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n","        super().__init__()\n","\n","        self.embedding = nn.Embedding(output_dim, embed_dim)\n","\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n","\n","        self.dropout = nn.Dropout(dropout_ratio)\n","\n","        # 밑의 것들이 Encoder과 다르게 Decoder에 추가되었다. Encoder와 구조적으로 다른 부분이다. \n","        self.output_dim = output_dim\n","        self.fc_out = nn.Linear(hidden_dim, output_dim) # hidden_dim == input, output_dim == output\n","\n","    \n","    # 디코더는 현재까지 출력된 문장에 대한 정보를 입력으로 받아 타겟 문장을 반환 \n","    def forward(self, input, hidden, cell):\n","        '''\n","        input:[배치 크기]. 단어의 개수는 항상 1개이도록 구현\n","        hidden:[레이어 개수, 배치 크기, 히든 차원]\n","        cell=context:[레이어 개수, 배치 크기, 히든 차원]\n","        '''\n","        input = input.unsqueeze(0)  # input:[단어 개수=1, 배치 크기]\n","\n","        embedded = self.dropout(self.embedding(input))  # embedded:[단어 개수, 배치 크기, 임베딩 차원]\n","\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","        # output:[단어 개수 = 1, 배치 크기, 히든 차원]. 현재 단어의 출력 정보\n","        # hidden:[레이어 개수, 배치 크기, 히든 차원]. 현재까지의 모든 단어의 정보\n","        # cell:[레이어 개수, 배치 크기, 히든 차원]. 현재까지의 모든 단어의 정보\n","\n","        # 단어 개수는 어차피 1개이므로 차원 제거 \n","        prediction = self.fc_out(output.squeeze(0))\n","        # prediction:[배치 크기, 출력 차원]\n","\n","        # prediction=현재 출력 단어, hidden=현재까지의 모든 단어 정보, cell=현재까지의 모든 단어의 정보 \n","        return prediction, hidden, cell "],"metadata":{"id":"EHgmtCkXWOfO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Seq2Seq\n","\n","- 앞서 정의한 인코더와 디코더를 가지고 있는 하나의 아키텍쳐임\n","- 인코더: 주어진 소스 문장을 context vector로 인코딩 \n","- 디코더: 주어진 context vector를 타겟 문장으로 디코딩 \n","- teacher forcing: 디코더의 predicition을 다음 입력으로 사용하지 않고, ground-truth를 다음 입력으로 사용한다."],"metadata":{"id":"9sbxIwUXfuld"}},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder \n","        self.device  = device \n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","\n","        # 먼저 인코더를 거쳐 context vector를 추출 \n","        hidden, cell = self.encoder(src)\n","\n","        # decoder의 최종 결과를 담을 텐서 객체 만들기 \n","        trg_len = trg.shape[0]  # 단어 갯수 \n","        batch_size = trg.shape[1] # 배치 크기 \n","        trg_vocab_size = self.decoder.output_dim # 출력 차원 \n","\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        input = trg[0, :]\n","\n","        # 타겟 단어의 개수만큼 반복하여 디코더에 forwarding\n","        for t in range(1, trg_len):\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","\n","            outputs[t] = output # FC를 거쳐서 나온 현재의 출력 단어 정보 \n","            top1 = output.argmax(1) # 가장 확률이 높은 단어의 인덱스를 추출 \n","\n","            # teacher_forcing_ratio: 학습할 때 ground truth를 사용하는 비율 \n","            teacher_force = random.random() < teacher_forcing_ratio \n","            input = trg[t] if teacher_force else top1 \n","\n","        return outputs "],"metadata":{"id":"30zLmAPUgodi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training"],"metadata":{"id":"8L9QFxXYorDy"}},{"cell_type":"code","source":["INPUT_DIM  = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","\n","ENCODER_EMBED_DIM = 256\n","DECODER_EMBED_DIM = 256\n","HIDDEN_DIM = 512\n","N_LAYERS = 2\n","ENC_DROPOUT_RATIO = 0.5\n","DEC_DROPOUT_RATIO = 0.5"],"metadata":{"id":"S43uVEpEosi7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(INPUT_DIM)\n","print(OUTPUT_DIM)"],"metadata":{"id":"QNquFw-YouEL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642658809620,"user_tz":-540,"elapsed":20,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"f2bc88fd-693e-4f39-b02a-61e07674d792"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7855\n","5893\n"]}]},{"cell_type":"code","source":["# 인코더(encoder)와 디코더(decoder) 객체 선언\n","enc = Encoder(INPUT_DIM,  ENCODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT_RATIO)\n","dec = Decoder(OUTPUT_DIM, DECODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT_RATIO)\n","\n","# Seq2Seq 객체 선언\n","model = Seq2Seq(enc, dec, device).to(device)"],"metadata":{"id":"bxpW4VlZozPV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 논문의 내용대로 u(-0.08, 0.08)의 값으로 모델 가중치 파라미터 초기화 \n","def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)\n","        \n","model.apply(init_weights)"],"metadata":{"id":"qmCIMkZyo8qP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642658810108,"user_tz":-540,"elapsed":7,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"895d8691-eef4-4e4b-9c68-f3558d7492d4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(7855, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(5893, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","# Adam optimizer로 학습 최적화\n","optimizer = optim.Adam(model.parameters())\n","\n","# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"],"metadata":{"id":"RImhBLM_o8sU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion, clip):\n","    model.train()\n","    epoch_loss = 0 \n","\n","    for i, batch in enumerate(iterator):\n","        src = batch.src \n","        trg = batch.trg \n","\n","        optimizer.zero_grad()\n","\n","        output = model(src, trg)\n","\n","        output_dim = output.shape[-1]\n","\n","        # 출력 단어의 인덱스 0은 사용하지 않음\n","        output = output[1:].view(-1, output_dim)\n","\n","        trg = trg[1:].view(-1)  # trg = [(타겟 단어의 개수 - 1) * batch size]\n","\n","        loss = criterion(output, trg)\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"],"metadata":{"id":"AazS4x9Bo8vn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, iterator, criterion):\n","    model.eval()\n","    epoch_loss = 0 \n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            src = batch.src \n","            trg = batch.trg \n","\n","            output = model(src, trg, 0) # 0 inidicates no teacher forcing \n","\n","            output_dim = output.shape[-1]\n","\n","            # 출력 단어의 인덱스 0은 사용하지 않는다. \n","            # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n","            output = output[1:].view(-1, output_dim)   \n","\n","            # trg = [(타겟 단어의 개수 - 1) * batch size]\n","            trg = trg[1:].view(-1)\n","\n","            loss = criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"],"metadata":{"id":"1exQdmTro8xz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time \n","    elapsed_mins = int(elapsed_time / 60)\n","\n","    return elapsed_mins"],"metadata":{"id":"-IBcLcc-YgAs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import math\n","import random"],"metadata":{"id":"KQ_j55NE_-mk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["N_EPOCHS = 20\n","CLIP = 1\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    start_time = time.time() # 시작 시간 기록\n","\n","\n","    print(f\"Training epoch-{epoch} started\")\n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","\n","    print(f\"Validation epoch-{epoch} started\")\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","\n","    end_time = time.time() # 종료 시간 기록\n","    epoch_mins  = epoch_time(start_time, end_time)\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'seq2seq.pt')\n","\n","    print(f'Epoch: {(epoch + 1):02} | Time: {epoch_mins}m')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n","    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')\n"],"metadata":{"id":"wn598x3nZDAg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from google.colab import files \n","\n","# files.download('seq2seq.pt')"],"metadata":{"id":"6hVk81JoZDCB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load('seq2seq.pt'))\n","\n","test_loss = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3G21-rGto-u","executionInfo":{"status":"ok","timestamp":1642659541931,"user_tz":-540,"elapsed":701,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"1a3e4b11-b820-496b-846f-a1bccee7c745"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 3.730 | Test PPL: 41.697\n"]}]},{"cell_type":"markdown","source":["Use my own data"],"metadata":{"id":"4pI_p_2hAGtr"}},{"cell_type":"code","source":["# 번역(translation) 함수\n","def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n","    model.eval() # 평가 모드\n","\n","    tokens = [token.lower() for token in sentence]\n","\n","    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n","    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n","    print(f\"전체 소스 토큰: {tokens}\")\n","\n","    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n","    print(f\"소스 문장 인덱스: {src_indexes}\")\n","\n","    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n","\n","    # 인코더(endocer)에 소스 문장을 넣어 문맥 벡터(context vector) 계산\n","    with torch.no_grad():\n","        hidden, cell = model.encoder(src_tensor)    # hidden, cell == context vector\n","\n","    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n","    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n","\n","    for i in range(max_len):\n","        # 이전에 출력한 단어가 현재 단어로 입력될 수 있도록\n","        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n","\n","        with torch.no_grad():\n","            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n","\n","        pred_token = output.argmax(1).item()\n","        trg_indexes.append(pred_token) # 출력 문장에 더하기\n","\n","        # <eos>를 만나는 순간 끝\n","        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n","            break\n","\n","    # 각 출력 단어 인덱스를 실제 단어로 변환\n","    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n","\n","    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n","    return trg_tokens[1:]"],"metadata":{"id":"ZSCDXLcBtpAT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_idx = 10\n","\n","src = vars(test_dataset.examples[example_idx])['src']\n","trg = vars(test_dataset.examples[example_idx])['trg']\n","\n","print(\"Target:\", trg)\n","print()\n","\n","print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kKN5G7DxtpCH","executionInfo":{"status":"ok","timestamp":1642659972375,"user_tz":-540,"elapsed":6,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"810e8a9d-68f5-4478-d8be-f2c8beb02c1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Target: ['a', 'mother', 'and', 'her', 'young', 'song', 'enjoying', 'a', 'beautiful', 'day', 'outside', '.']\n","\n","전체 소스 토큰: ['<sos>', '.', 'freien', 'im', 'tag', 'schönen', 'einen', 'genießen', 'sohn', 'kleiner', 'ihr', 'und', 'mutter', 'eine', '<eos>']\n","소스 문장 인덱스: [2, 4, 88, 20, 200, 780, 19, 565, 624, 70, 134, 10, 364, 8, 3]\n","모델 출력 결과: a girl and her mother are enjoying a stroll in the woods . <eos>\n"]}]},{"cell_type":"code","source":["src = tokenize_de(\"Guten Abend.\")\n","trg = \"Good evening\"\n","\n","print(\"Target:\", trg)\n","print()\n","\n","print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oKUcZa3QtpHi","executionInfo":{"status":"ok","timestamp":1642659974561,"user_tz":-540,"elapsed":8,"user":{"displayName":"J Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14707665891089999345"}},"outputId":"4a159688-da5b-415f-801c-388aaf62bea0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Target: Good evening\n","\n","전체 소스 토큰: ['<sos>', '.', 'abend', 'guten', '<eos>']\n","소스 문장 인덱스: [2, 4, 1163, 3799, 3]\n","모델 출력 결과: <unk> . <eos>\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"I8Q7m0QOGBsX"},"execution_count":null,"outputs":[]}]}