{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Q-Network v2",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+XR18g6gODG4ehYNP8Eo6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7201krap/KNU_RA/blob/main/Deep_Q_Network_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WG254OAN5yC1"
      },
      "outputs": [],
      "source": [
        "# NIPS 2013\n",
        "\n",
        "from torch import nn\n",
        "import torch\n",
        "import gym\n",
        "from collections import deque\n",
        "import itertools\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GAMMA = 0.99                                                                    # decaying rate\n",
        "BATCH_SIZE = 32                                                                 # How many transitions we are going to sample from the replay buffer when we are computing our gradients\n",
        "BUFFER_SIZE = 50000                                                             # Maximum number of transition we are going to store\n",
        "MIN_REPLAY_SIZE = 1000                                                          # How many transition we want in the replay buffer before we start computing gradients\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_END = 0.02\n",
        "EPSILON_DECAY = 10000"
      ],
      "metadata": {
        "id": "DlU8xFGD6AEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "\n",
        "        in_features = int(np.prod(env.observation_space.shape))\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, env.action_space.n)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def act(self, state):\n",
        "\n",
        "        state_t = torch.as_tensor(state, dtype=torch.float32)\n",
        "\n",
        "        q_values = self.forward(state_t.unsqueeze(0))                           # 'q_values' outputs two values (left or right)\n",
        "\n",
        "        max_q_index = torch.argmax(q_values, dim=1)[0]                          # find an index that corresponds to the maximum value  \n",
        "\n",
        "        action = max_q_index.detach().item()                                    # 0 or 1\n",
        "\n",
        "        return action                                                           # 0 or 1"
      ],
      "metadata": {
        "id": "94gM5-9S6AOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
        "reward_buffer = deque([0.0], maxlen=100)\n",
        "episode_reward = 0.0\n",
        "\n",
        "target_net = Network(env)\n",
        "\n",
        "optimizer = torch.optim.Adam(target_net.parameters(), lr=5e-4)"
      ],
      "metadata": {
        "id": "t1f6m0XQ6CVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Replay Buffer\n",
        "# 최소 MIN_REPLAY_SIZE 만큼의 transition을 buffer에 넣고 시작한다. \n",
        "state = env.reset()\n",
        "\n",
        "for _ in range(MIN_REPLAY_SIZE):\n",
        "\n",
        "    action = env.action_space.sample()\n",
        "    new_state, reward, done, _ = env.step(action)\n",
        "    transition = (state, action, reward, done, new_state)\n",
        "    replay_buffer.append(transition)\n",
        "    state = new_state\n",
        "\n",
        "    if done:                                                                    # 게임이 끝나게 되면 다시 리셋해주고 버퍼를 쌓아준다. \n",
        "        state = env.reset()"
      ],
      "metadata": {
        "id": "cvXJkWQP6CXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Training Loop\n",
        "\n",
        "state = env.reset()                                                             # state의 예시: [-0.01713841 -0.00705756 -0.04146662 -0.04927411]\n",
        "\n",
        "for step in itertools.count():                                                  # step starts from 0 and increases by 1 until it meets a break condition. This is same as 'While True' loop\n",
        "\n",
        "    epsilon = np.interp(step, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END]) # epsilon value는 EPSILON_START에서 시작해서 EPSILON_END까지 step이 흘러갈수록 점점 더 감소한다. \n",
        "\n",
        "    random_sample = random.random()\n",
        "\n",
        "    if random_sample <= epsilon:                                                # random_sample의 값이 epsilon보다 작으면, random한 action을 취하고, 그렇지 않다면 target_net에 현재 state를 넣어 가장 좋은 act를 가져온다. \n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = target_net.act(state)\n",
        "\n",
        "    new_state, reward, done, _ = env.step(action)\n",
        "    transition = (state, action, reward, done, new_state)\n",
        "    replay_buffer.append(transition)                                            # step 1번에 transition 1번이 append된다. \n",
        "    state = new_state\n",
        "\n",
        "    episode_reward  = episode_reward + reward\n",
        "\n",
        "    if done:                                                                    # 게임이 끝나면(막대기가 넘어지면), done값이 True가 된다. \n",
        "        state = env.reset()                                                     # 끝난다면 env를 reset해주고, \n",
        "        reward_buffer.append(episode_reward)                                    # reward_buffer에 episode_reward를 append해준다. \n",
        "        episode_reward = 0.0\n",
        "\n",
        "    # # -------------------------- TEST --------------------------\n",
        "    # # After solved, watch it play\n",
        "    # if len(reward_buffer) >= 100:\n",
        "    #     if np.mean(reward_buffer) >= 195:\n",
        "    #         while True:\n",
        "    #             action = target_net.act(state)\n",
        "\n",
        "    #             state, _, done, _ = env.step(action)\n",
        "    #             env.render()\n",
        "    #             if done:\n",
        "    #                 env.reset()\n",
        "    # # -------------------------- TEST --------------------------\n",
        "\n",
        "    # Start Gradient Step\n",
        "    transitions = random.sample(replay_buffer, BATCH_SIZE)                      # replay_buffer에서 batch_size만큼의 sample을 가져온다. \n",
        "\n",
        "    states     = np.asarray([t[0] for t in transitions])                        # len(states) == 32\n",
        "    actions    = np.asarray([t[1] for t in transitions])                        # len(actions) == 32\n",
        "    rewards    = np.asarray([t[2] for t in transitions])                        # len(rewards) == 32    \n",
        "    dones      = np.asarray([t[3] for t in transitions])                        # len(dones) == 32    \n",
        "    new_states = np.asarray([t[4] for t in transitions])                        # len(new_states) == 32\n",
        "\n",
        "    # print(\"states -->\", states[0])\n",
        "    # print(\"actions -->\", actions[0])\n",
        "    # print(\"rewards -->\", rewards[0])\n",
        "    # print(\"dones -->\", dones[0])\n",
        "    # print(\"new_states -->\", new_states[0])\n",
        "    \n",
        "    states_t     = torch.as_tensor(states, dtype=torch.float32)\n",
        "    actions_t    = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
        "    rewards_t    = torch.as_tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
        "    dones_t      = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
        "    new_states_t = torch.as_tensor(new_states, dtype=torch.float32)\n",
        "\n",
        "    # Compute Targets\n",
        "    target_q_values = target_net.forward(new_states_t)\n",
        "    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]           \n",
        "    targets = rewards_t + GAMMA * (1 - dones_t) * max_target_q_values           # 32 targets are computed. See Algorithm 1 of Human-level control through deep reinforcement learning (Nature14236). \n",
        "\n",
        "    # Compute Loss\n",
        "    q_values = target_net.forward(states_t)                             \n",
        "    action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)      # 32 action values are computed \n",
        "    loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
        "\n",
        "    # Gradient Descent\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Logging\n",
        "    if step % 1000 == 0:\n",
        "        print()\n",
        "        print('Step', step)\n",
        "        print('Avg Reward', np.mean(reward_buffer))                             # maximum length of reward_buffer is 100. Therefore, np.mean(reward_buffer) averages lastest 100 rewards"
      ],
      "metadata": {
        "id": "NlZsaKb-6CZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "036fb0c7-f477-4794-a36c-bf3be8716d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 0\n",
            "Avg Reward 0.0\n",
            "\n",
            "Step 1000\n",
            "Avg Reward 26.135135135135137\n",
            "\n",
            "Step 2000\n",
            "Avg Reward 23.607142857142858\n",
            "\n",
            "Step 3000\n",
            "Avg Reward 23.76\n",
            "\n",
            "Step 4000\n",
            "Avg Reward 23.02\n",
            "\n",
            "Step 5000\n",
            "Avg Reward 26.08\n",
            "\n",
            "Step 6000\n",
            "Avg Reward 26.89\n",
            "\n",
            "Step 7000\n",
            "Avg Reward 28.77\n",
            "\n",
            "Step 8000\n",
            "Avg Reward 29.9\n",
            "\n",
            "Step 9000\n",
            "Avg Reward 33.48\n",
            "\n",
            "Step 10000\n",
            "Avg Reward 37.46\n",
            "\n",
            "Step 11000\n",
            "Avg Reward 42.82\n",
            "\n",
            "Step 12000\n",
            "Avg Reward 47.6\n",
            "\n",
            "Step 13000\n",
            "Avg Reward 52.95\n",
            "\n",
            "Step 14000\n",
            "Avg Reward 56.73\n",
            "\n",
            "Step 15000\n",
            "Avg Reward 59.93\n",
            "\n",
            "Step 16000\n",
            "Avg Reward 61.77\n",
            "\n",
            "Step 17000\n",
            "Avg Reward 63.23\n",
            "\n",
            "Step 18000\n",
            "Avg Reward 64.33\n",
            "\n",
            "Step 19000\n",
            "Avg Reward 65.39\n",
            "\n",
            "Step 20000\n",
            "Avg Reward 66.92\n"
          ]
        }
      ]
    }
  ]
}