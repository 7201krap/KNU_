{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WG254OAN5yC1"
   },
   "outputs": [],
   "source": [
    "# Human-level control through deep reinforcement learning\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlU8xFGD6AEF"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99                                                                    # decaying rate\n",
    "BATCH_SIZE = 32                                                                 # How many transitions we are going to sample from the replay buffer when we are computing our gradients\n",
    "BUFFER_SIZE = 50000                                                             # Maximum number of transition we are going to store\n",
    "MIN_REPLAY_SIZE = 1000                                                          # How many transition we want in the replay buffer before we start computing gradients\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY = 10000\n",
    "TARGET_UPDATE_FREQ = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94gM5-9S6AOb"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "\n",
    "        in_features = int(np.prod(env.observation_space.shape))\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, env.action_space.n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        state_t = torch.as_tensor(state, dtype=torch.float32)\n",
    "\n",
    "        q_values = self.forward(state_t.unsqueeze(0))                           # 'q_values' outputs two values (left or right)\n",
    "\n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]                          # find an index that corresponds to the maximum value  \n",
    "\n",
    "        action = max_q_index.detach().item()                                    # 0 or 1\n",
    "\n",
    "        return action                                                           # 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1f6m0XQ6CVU"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "reward_buffer = deque([0.0], maxlen=100)\n",
    "episode_reward = 0.0\n",
    "\n",
    "online_net = Network(env)\n",
    "target_net = Network(env)\n",
    "\n",
    "target_net.load_state_dict(online_net.state_dict())                             # we set the target net parameters equal to the online network parameters\n",
    "\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvXJkWQP6CXZ"
   },
   "outputs": [],
   "source": [
    "# Initialize Replay Buffer\n",
    "# 최소 MIN_REPLAY_SIZE 만큼의 transition을 buffer에 넣고 시작한다. \n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "\n",
    "    action = env.action_space.sample()\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    transition = (state, action, reward, done, new_state)\n",
    "    replay_buffer.append(transition)\n",
    "    state = new_state\n",
    "\n",
    "    if done:                                                                    # 게임이 끝나게 되면 다시 리셋해주고 버퍼를 쌓아준다. \n",
    "        state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NlZsaKb-6CZP",
    "outputId": "cdefb313-97ee-4d72-fea3-2791a01db18d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0\n",
      "Avg Reward 0.0\n",
      "\n",
      "Step 1000\n",
      "Avg Reward 22.613636363636363\n",
      "\n",
      "Step 2000\n",
      "Avg Reward 23.22093023255814\n",
      "\n",
      "Step 3000\n",
      "Avg Reward 22.02\n",
      "\n",
      "Step 4000\n",
      "Avg Reward 23.32\n",
      "\n",
      "Step 5000\n",
      "Avg Reward 24.34\n",
      "\n",
      "Step 6000\n",
      "Avg Reward 28.43\n",
      "\n",
      "Step 7000\n",
      "Avg Reward 33.95\n",
      "\n",
      "Step 8000\n",
      "Avg Reward 41.38\n",
      "\n",
      "Step 9000\n",
      "Avg Reward 48.58\n",
      "\n",
      "Step 10000\n",
      "Avg Reward 57.37\n",
      "\n",
      "Step 11000\n",
      "Avg Reward 66.22\n",
      "\n",
      "Step 12000\n",
      "Avg Reward 75.51\n",
      "\n",
      "Step 13000\n",
      "Avg Reward 84.27\n",
      "\n",
      "Step 14000\n",
      "Avg Reward 92.15\n",
      "\n",
      "Step 15000\n",
      "Avg Reward 100.31\n",
      "\n",
      "Step 16000\n",
      "Avg Reward 109.48\n",
      "\n",
      "Step 17000\n",
      "Avg Reward 116.96\n",
      "\n",
      "Step 18000\n",
      "Avg Reward 124.8\n",
      "\n",
      "Step 19000\n",
      "Avg Reward 132.97\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-07f0185e7132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Update Target Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main Training Loop\n",
    "\n",
    "state = env.reset()                                                             # state의 예시: [-0.01713841 -0.00705756 -0.04146662 -0.04927411]\n",
    "\n",
    "for step in itertools.count():                                                  # step starts from 0 and increases by 1 until it meets a break condition. This is same as 'While True' loop\n",
    "\n",
    "    epsilon = np.interp(step, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END]) # epsilon value는 EPSILON_START에서 시작해서 EPSILON_END까지 step이 흘러갈수록 점점 더 감소한다. \n",
    "\n",
    "    random_sample = random.random()\n",
    "\n",
    "    if random_sample <= epsilon:                                                # random_sample의 값이 epsilon보다 작으면, random한 action을 취하고, 그렇지 않다면 online_net에 현재 state를 넣어 가장 좋은 act를 가져온다. \n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = online_net.act(state)\n",
    "\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    transition = (state, action, reward, done, new_state)\n",
    "    replay_buffer.append(transition)                                            # step 1번에 transition 1번이 append된다. \n",
    "    state = new_state\n",
    "\n",
    "    episode_reward  = episode_reward + reward\n",
    "\n",
    "    if done:                                                                    # 게임이 끝나면(막대기가 넘어지면), done값이 True가 된다. \n",
    "        state = env.reset()                                                     # 끝난다면 env를 reset해주고, \n",
    "        reward_buffer.append(episode_reward)                                    # reward_buffer에 episode_reward를 append해준다. \n",
    "        episode_reward = 0.0\n",
    "\n",
    "    # # -------------------------- TEST --------------------------\n",
    "    # # After solved, watch it play\n",
    "    # if len(reward_buffer) == 100:\n",
    "    #     if np.mean(reward_buffer) >= 195:\n",
    "    #         while True:\n",
    "    #             action = online_net.act(state)\n",
    "\n",
    "    #             action, _, done, _ = env.step(action)\n",
    "    #             env.render()\n",
    "    #             if done:\n",
    "    #                 env.reset()\n",
    "    # # -------------------------- TEST --------------------------\n",
    "\n",
    "    # Start Gradient Step\n",
    "    transitions = random.sample(replay_buffer, BATCH_SIZE)                      # replay_buffer에서 batch_size만큼의 sample을 가져온다. \n",
    "\n",
    "    states     = np.asarray([t[0] for t in transitions])                        # len(states) == 32\n",
    "    actions    = np.asarray([t[1] for t in transitions])                        # len(actions) == 32\n",
    "    rewards    = np.asarray([t[2] for t in transitions])                        # len(rewards) == 32    \n",
    "    dones      = np.asarray([t[3] for t in transitions])                        # len(dones) == 32    \n",
    "    new_states = np.asarray([t[4] for t in transitions])                        # len(new_states) == 32\n",
    "\n",
    "    # print(\"states -->\", states[0])\n",
    "    # print(\"actions -->\", actions[0])\n",
    "    # print(\"rewards -->\", rewards[0])\n",
    "    # print(\"dones -->\", dones[0])\n",
    "    # print(\"new_states -->\", new_states[0])\n",
    "    \n",
    "    states_t     = torch.as_tensor(states, dtype=torch.float32)\n",
    "    actions_t    = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rewards_t    = torch.as_tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_t      = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_states_t = torch.as_tensor(new_states, dtype=torch.float32)\n",
    "\n",
    "    # Compute Targets\n",
    "    target_q_values = target_net.forward(new_states_t)\n",
    "    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]           \n",
    "    targets = rewards_t + GAMMA * (1 - dones_t) * max_target_q_values           # 32 targets are computed. See Algorithm 1 of Human-level control through deep reinforcement learning (Nature14236). \n",
    "\n",
    "    # Compute Loss\n",
    "    q_values = online_net.forward(states_t)                             \n",
    "    action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)      # 32 action values are computed \n",
    "    loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
    "\n",
    "    # Gradient Descent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update Target Network\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())                     # we set the target net parameters equal to the online network parameters\n",
    "\n",
    "    # Logging\n",
    "    if step % 1000 == 0:\n",
    "        print()\n",
    "        print('Step', step)\n",
    "        print('Avg Reward', np.mean(reward_buffer))                             # maximum length of reward_buffer is 100. Therefore, np.mean(reward_buffer) averages lastest 100 rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtbe97UBm8Lo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Q-Network v2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
