{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KQ5Ett0shGg"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.autograd as autograd \n",
    "import numpy as np \n",
    "import random \n",
    "from collections import deque\n",
    "import gym\n",
    "\n",
    "from collections import deque\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGqKjyNRtHdf"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, env, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        in_features = int(np.prod(env.observation_space.shape))\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, env.action_space.n)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        qvals = self.net(state)\n",
    "        return qvals\n",
    "\n",
    "    def act(self, state):\n",
    "        state_t = torch.as_tensor(state, dtype=torch.float32)\n",
    "        q_values = self.forward(state_t.unsqueeze(0))                           # 'q_values' outputs two values (left or right)\n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]                          # find an index that corresponds to the maximum value  \n",
    "        action = max_q_index.detach().item()                                    # 0 or 1\n",
    "        return action    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7aCXiCkx5tx"
   },
   "outputs": [],
   "source": [
    "class SumTree():\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros( 2*capacity - 1 )\n",
    "        self.data = np.zeros( capacity, dtype=object )\n",
    "        self.n_entries = 0\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3s2TPZlUvB00"
   },
   "outputs": [],
   "source": [
    "class PrioritizedBuffer:\n",
    "\n",
    "    def __init__(self, max_size):\n",
    "        self.sum_tree = SumTree(max_size)\n",
    "        self.current_length = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "\n",
    "        priority = 1.0 if self.current_length is 0 else self.sum_tree.tree.max()# 이 부분이 뭔가 이상하다,,,\n",
    "        self.current_length = self.current_length + 1                           # current_length는 현재 PrioritizedBuffer의 크기를 추적한다. \n",
    "        \n",
    "        experience = (state, action, np.array([reward]), next_state, done)      # priority = td_error ** self.alpha \n",
    "        self.sum_tree.add(priority, experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch_idx, batch, priorities = [], [], []\n",
    "        \n",
    "        segment = self.sum_tree.total() / batch_size\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.sum_tree.get(s)\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            batch_idx.append(idx)\n",
    "\n",
    "        sampling_probabilites = priorities / (self.sum_tree.total() + EP)\n",
    "        IS_weights = np.power(self.sum_tree.n_entries * sampling_probabilites, -BETA)\n",
    "        IS_weights /= IS_weights.max()\n",
    "        # len(IS_weights) == 32\n",
    "\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "\n",
    "        for transition in batch:\n",
    "            state, action, reward, next_state, done = transition\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "\n",
    "        return (state_batch, action_batch, reward_batch, next_state_batch, done_batch), batch_idx, IS_weights\n",
    "\n",
    "    def update_priority(self, idx, td_error):\n",
    "        priority = (np.abs(td_error) + EP) ** ALPHA                             # add epsilon to priority value\n",
    "        self.sum_tree.update(idx, priority)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.current_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBizkrBV5J9b"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 10000\n",
    "REPLAY_BUFFER = PrioritizedBuffer(BUFFER_SIZE)\n",
    "LEARNING_RATE = 5e-4\n",
    "GAMMA = 0.99 \n",
    "\n",
    "# annealing hyper-parameters\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY = 20000\n",
    "\n",
    "ALPHA = 0.7\n",
    "\n",
    "BETA = 0\n",
    "BETA_START = 0.5\n",
    "BETA_END = 1.0\n",
    "BETA_ANNEAL = 20000\n",
    "# annealing hyper-parameters\n",
    "\n",
    "EP = 1e-6                                                                       # division by 0 방지 \n",
    "\n",
    "EPISODE_REWARD = 0.0\n",
    "REWARD_BUFFER  = deque([0.0], maxlen=100)     \n",
    "MIN_REPLAY_SIZE = 100                                  \n",
    "\n",
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "online_net = DQN(env, env.action_space.n)                                       # DDQN implementation\n",
    "target_net = DQN(env, env.action_space.n)                                       # DDQN implementation\n",
    "\n",
    "target_net.load_state_dict(online_net.state_dict())                             # target_net parameters <- online_net parameters\n",
    "\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 940
    },
    "executionInfo": {
     "elapsed": 17563,
     "status": "error",
     "timestamp": 1654610716150,
     "user": {
      "displayName": "J Park",
      "userId": "14707665891089999345"
     },
     "user_tz": -540
    },
    "id": "ie5ZUgANHRwo",
    "outputId": "3a20bef4-da92-4328-f120-8c852003f14f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1000\n",
      "Avg Reward 18.846153846153847\n",
      "Loss tensor(0.0037, grad_fn=<MeanBackward0>)\n",
      "BETA 0.525\n",
      "ALPHA 0.7\n",
      "\n",
      "Step 2000\n",
      "Avg Reward 20.418367346938776\n",
      "Loss tensor(0.0171, grad_fn=<MeanBackward0>)\n",
      "BETA 0.55\n",
      "ALPHA 0.7\n",
      "\n",
      "Step 3000\n",
      "Avg Reward 23.71\n",
      "Loss tensor(0.0997, grad_fn=<MeanBackward0>)\n",
      "BETA 0.575\n",
      "ALPHA 0.7\n",
      "\n",
      "Step 4000\n",
      "Avg Reward 26.17\n",
      "Loss tensor(0.0984, grad_fn=<MeanBackward0>)\n",
      "BETA 0.6\n",
      "ALPHA 0.7\n",
      "\n",
      "Step 5000\n",
      "Avg Reward 30.41\n",
      "Loss tensor(0.1591, grad_fn=<MeanBackward0>)\n",
      "BETA 0.625\n",
      "ALPHA 0.7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e231333ecf12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# update priorities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mREPLAY_BUFFER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_priority\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m                           \u001b[0;31m# TD error의 절댓값으로 update해준다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1f34820e1757>\u001b[0m in \u001b[0;36mupdate_priority\u001b[0;34m(self, idx, td_error)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_priority\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mpriority\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_error\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mEP\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mALPHA\u001b[0m                             \u001b[0;31m# add epsilon to priority value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriority\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0c621639cda4>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, idx, p)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0c621639cda4>\u001b[0m in \u001b[0;36m_propagate\u001b[0;34m(self, idx, change)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0c621639cda4>\u001b[0m in \u001b[0;36m_propagate\u001b[0;34m(self, idx, change)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0c621639cda4>\u001b[0m in \u001b[0;36m_propagate\u001b[0;34m(self, idx, change)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for step in itertools.count():  \n",
    "    \n",
    "    epsilon = np.interp(step, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END]) # annealing beta from EPSILON_START to EPSILON_END\n",
    "    BETA    = np.interp(step, [0, BETA_ANNEAL],   [BETA_START, BETA_END])       # annealing beta from BETA_START to BETA_END\n",
    "\n",
    "    random_sample = random.random()\n",
    "    \n",
    "    if random_sample <= epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = online_net.act(state)\n",
    "\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    REPLAY_BUFFER.push(state, action, reward, next_state, done)\n",
    "\n",
    "    state = next_state\n",
    "    EPISODE_REWARD += reward\n",
    "\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        REWARD_BUFFER.append(EPISODE_REWARD)\n",
    "        EPISODE_REWARD = 0.0\n",
    "\n",
    "    # burn in steps. The length of REPLAY_BUFFER should be greater than the BATCH_SIZE.\n",
    "    # This is because we are going to make BATCH from REPLAY_BUFFER\n",
    "    if len(REPLAY_BUFFER) > BATCH_SIZE:\n",
    "\n",
    "        # sample transitions from REPLAY_BUFFER\n",
    "        transitions, idxs, IS_weights = REPLAY_BUFFER.sample(BATCH_SIZE)\n",
    "        states, actions, rewards, next_states, dones = transitions\n",
    "\n",
    "        # preprocess the followings: states, actions, rewards, next_states, dones, IS_weights\n",
    "        states = torch.FloatTensor(np.array(states)).to(device)\n",
    "        actions = torch.LongTensor(np.array(actions)).to(device)\n",
    "        rewards = torch.FloatTensor(np.array(rewards)).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "        dones = torch.FloatTensor(np.array(dones)).to(device)\n",
    "        IS_weights = torch.FloatTensor(np.array(IS_weights)).to(device)\n",
    "\n",
    "        # reshape tensors to appropriate formats\n",
    "        states  = torch.reshape(states,  (BATCH_SIZE, 4))\n",
    "        actions = torch.reshape(actions, (BATCH_SIZE, 1))\n",
    "        rewards = torch.reshape(rewards, (BATCH_SIZE, 1))\n",
    "        next_states = torch.reshape(next_states, (BATCH_SIZE, 4))\n",
    "        dones = torch.reshape(dones, (BATCH_SIZE, 1))\n",
    "        IS_weights = torch.reshape(IS_weights, (BATCH_SIZE, 1))\n",
    "\n",
    "        # Compute Targets\n",
    "        online_with_new_states = online_net.forward(next_states)\n",
    "        argmax_online_with_new_states = online_with_new_states.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        offline_with_new_states = target_net.forward(next_states)\n",
    "        target_q_vals = torch.gather(input=offline_with_new_states, dim=1, index=argmax_online_with_new_states)\n",
    "        targets = rewards + GAMMA * (1 - dones) * target_q_vals  \n",
    "\n",
    "        # Compute Loss\n",
    "        q_values = online_net.forward(states)\n",
    "        action_q_values = torch.gather(input=q_values, dim=1, index=actions)  \n",
    "        errors = torch.abs(action_q_values - targets).data.numpy()              # errors == TD error의 절댓값\n",
    "\n",
    "        td_errors = torch.pow(action_q_values - targets, 2) * IS_weights        # MSE Loss\n",
    "        td_errors_mean = td_errors.mean()\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        td_errors_mean.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update priorities\n",
    "        for idx, error in zip(idxs, errors):\n",
    "            REPLAY_BUFFER.update_priority(idx, error)                           # TD error의 절댓값으로 update해준다. \n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            target_net.load_state_dict(online_net.state_dict())        \n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print()\n",
    "            print('Step', step)\n",
    "            print('Avg Reward', np.mean(REWARD_BUFFER))                         # maximum length of reward_buffer is 100. Therefore, np.mean(reward_buffer) averages lastest 100 rewards\n",
    "            print('Loss', td_errors_mean)\n",
    "            print('BETA', BETA)\n",
    "            print('ALPHA', ALPHA)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPcltB0+8tZ4h27y+dv5vFF",
   "collapsed_sections": [],
   "name": "6_PER DDQN.ipynb",
   "provenance": [
    {
     "file_id": "1SVQ-SKchHlPti0lDZml4qmZEYIbyvWP_",
     "timestamp": 1654510951785
    },
    {
     "file_id": "1jcg7u3AOgrtMi0tYWaG1Qc32SiU_0P9B",
     "timestamp": 1654494409279
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
