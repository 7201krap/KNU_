{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6111,
     "status": "ok",
     "timestamp": 1654509417042,
     "user": {
      "displayName": "J Park",
      "userId": "14707665891089999345"
     },
     "user_tz": -540
    },
    "id": "8KQ5Ett0shGg"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.autograd as autograd \n",
    "import numpy as np \n",
    "import random \n",
    "from collections import deque\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1654509417043,
     "user": {
      "displayName": "J Park",
      "userId": "14707665891089999345"
     },
     "user_tz": -540
    },
    "id": "OGqKjyNRtHdf"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim[0], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        qvals = self.fc(state)\n",
    "        return qvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1654509417043,
     "user": {
      "displayName": "J Park",
      "userId": "14707665891089999345"
     },
     "user_tz": -540
    },
    "id": "f7aCXiCkx5tx"
   },
   "outputs": [],
   "source": [
    "class SumTree():\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros( 2*capacity - 1 )\n",
    "        self.data = np.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1654509417392,
     "user": {
      "displayName": "J Park",
      "userId": "14707665891089999345"
     },
     "user_tz": -540
    },
    "id": "3s2TPZlUvB00"
   },
   "outputs": [],
   "source": [
    "class PrioritizedBuffer:\n",
    "\n",
    "    def __init__(self, max_size, alpha=0.6, beta=0.4):\n",
    "        self.sum_tree = SumTree(max_size)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.current_length = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        priority = 1.0 if self.current_length is 0 else self.sum_tree.tree.max()\n",
    "        self.current_length = self.current_length + 1\n",
    "        #priority = td_error ** self.alpha\n",
    "        experience = (state, action, np.array([reward]), next_state, done)\n",
    "        self.sum_tree.add(priority, experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch_idx, batch, IS_weights = [], [], []\n",
    "        segment = self.sum_tree.total() / batch_size\n",
    "        p_sum = self.sum_tree.tree[0]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, data = self.sum_tree.get(s)\n",
    "\n",
    "            batch_idx.append(idx)\n",
    "            batch.append(data)\n",
    "            prob = p / p_sum\n",
    "            IS_weight = (self.sum_tree.total() * prob) ** (-self.beta)\n",
    "            IS_weights.append(IS_weight)\n",
    "\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "\n",
    "        for transition in batch:\n",
    "            state, action, reward, next_state, done = transition\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "\n",
    "        return (state_batch, action_batch, reward_batch, next_state_batch, done_batch), batch_idx, IS_weights\n",
    "\n",
    "    def update_priority(self, idx, td_error):\n",
    "        priority = td_error ** self.alpha\n",
    "        self.sum_tree.update(idx, priority)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.current_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1654513718372,
     "user": {
      "displayName": "J Park",
      "userId": "14707665891089999345"
     },
     "user_tz": -540
    },
    "id": "KJhf-Mms_MW2"
   },
   "outputs": [],
   "source": [
    "class PERAgent:\n",
    "\n",
    "    def __init__(self, env, use_conv=False, learning_rate=3e-4, gamma=0.99, buffer_size=10000):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.replay_buffer = PrioritizedBuffer(buffer_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")        \n",
    "\t\n",
    "        if use_conv:\n",
    "            self.model = ConvDQN(self.env.observation_space.shape, env.action_space.n).to(self.device)\n",
    "        else:\n",
    "            self.model = DQN(self.env.observation_space.shape, env.action_space.n).to(self.device)\n",
    "          \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        self.MSE_loss = nn.MSELoss()\n",
    "\n",
    "    def get_action(self, state, eps=0.0):\n",
    "        state = torch.FloatTensor(state).float().unsqueeze(0).to(self.device)\n",
    "        qvals = self.model.forward(state)\n",
    "        action = np.argmax(qvals.cpu().detach().numpy())\n",
    "        \n",
    "        if(np.random.rand() > eps):\n",
    "            return self.env.action_space.sample()\n",
    "          \n",
    "        return action\n",
    "\n",
    "    def _sample(self, batch_size):\n",
    "        return self.replay_buffer.sample(batch_size)\n",
    "\n",
    "    def _compute_TDerror(self, batch_size):\n",
    "        transitions, idxs, IS_weights = self._sample(batch_size)\n",
    "\n",
    "        # print(\"------------------------\")\n",
    "        # print(\"transitions\", transitions)\n",
    "        # print(\"------------------------\")\n",
    "\n",
    "        states, actions, rewards, next_states, dones = transitions\n",
    "\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        IS_weights = torch.FloatTensor(IS_weights).to(self.device)\n",
    "\n",
    "        print(\"----------asfd----------\")\n",
    "        print(\"states\", states)\n",
    "        print(\"actions\", actions)\n",
    "        print(\"rewards\", rewards)\n",
    "        print(\"next_states\", next_states)\n",
    "        print(\"dones\", dones)\n",
    "        print(\"IS_weights\", IS_weights)\n",
    "        print(\"------------asfd------------\")\n",
    "\n",
    "        curr_Q = self.model.forward(states).gather(1, actions.unsqueeze(1))\n",
    "        curr_Q = curr_Q.squeeze(1)\n",
    "        next_Q = self.model.forward(next_states)\n",
    "        max_next_Q = torch.max(next_Q, 1)[0]\n",
    "        expected_Q = rewards.squeeze(1) + self.gamma * max_next_Q\n",
    "\n",
    "        td_errors = torch.pow(curr_Q - expected_Q, 2) * IS_weights\n",
    "\n",
    "        return td_errors, idxs\n",
    "\n",
    "    def update(self, batch_size):\n",
    "        td_errors, idxs = self._compute_TDerror(batch_size)\n",
    "\n",
    "        # update model\n",
    "        td_errors_mean = td_errors.mean()\n",
    "\n",
    "        print(\"td_errors_mean\", td_errors_mean)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        td_errors_mean.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update priorities\n",
    "        for idx, td_error in zip(idxs, td_errors.cpu().detach().numpy()):\n",
    "            self.replay_buffer.update_priority(idx, td_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1654513720261,
     "user": {
      "displayName": "J Park",
      "userId": "14707665891089999345"
     },
     "user_tz": -540
    },
    "id": "ie5ZUgANHRwo"
   },
   "outputs": [],
   "source": [
    "def mini_batch_train(env, agent, max_episodes, max_steps, batch_size):\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if len(agent.replay_buffer) > batch_size:\n",
    "                agent.update(batch_size)   \n",
    "\n",
    "            if done or step == max_steps-1:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                print(\"Episode \" + str(episode) + \": \" + str(episode_reward))\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjqWDA8vGCwH"
   },
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "\n",
    "MAX_EPISODES = 1000\n",
    "MAX_STEPS = 500\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "env = gym.make(env_id)\n",
    "agent = PERAgent(env, use_conv=False)\n",
    "\n",
    "mini_batch_train(env, agent, MAX_EPISODES, MAX_STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1654509421374,
     "user": {
      "displayName": "J Park",
      "userId": "14707665891089999345"
     },
     "user_tz": -540
    },
    "id": "mxYH6ieIN0g3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJow1Sye8Q5KyDOYBOt+TM",
   "collapsed_sections": [],
   "name": "6_PER DQN (not DDQN).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
